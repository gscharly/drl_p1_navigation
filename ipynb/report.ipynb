{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75f724a-4e8d-464b-963d-9bc08fbaf663",
   "metadata": {},
   "source": [
    "# Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04de559-d97c-4301-875e-4384e8436539",
   "metadata": {},
   "source": [
    "The proposed solution uses the Deep Q-learning algorithm to solve the enviroment. This algorithm provides a solution when the state or action spaces have\n",
    "large dimensions and are continous. It is suitable for episodic tasks. This version of the algorithm uses Experience replay and Fixed Q-targets.\n",
    "\n",
    "__High level description__\n",
    "- The agent has 2 neural networks (local & target) that are randomly initialized to the same weights values. In this case, the NNs are fully connected with 3 dense layers. Inputs are\n",
    "states and the outputs are the action values.\n",
    "- For each episode, there will be T steps to take. An initial state is taken, and the algorithm can perform 2 actions: Sample or Learn.\n",
    "- When sampling, the algorithm will choose an action following the epsilon-greedy policy, and get the reward and following state from the environment. This 'experince' will\n",
    "be stored in a replay memory.\n",
    "- When learning, the algorithm will sample a batch of experiences from the replay memory. This helps decoupling consecutive steps. Then, it will update the weights of the local neural network, by comparing the target (Reward + max q value when using the target network) and the actual q value recorded by the experience. The target network will only be updated after some timesteps, reducing the correlation between the target and the NN's parameters.\n",
    "\n",
    "__Hyperparameters__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c192e-2eb2-4484-8c27-05ab68a6587d",
   "metadata": {},
   "source": [
    "- Discount factor: 0.99\n",
    "- Epsilon decay starting in 1 and with decay factor 0.995\n",
    "- Experience replay buffer size: 64\n",
    "- Neural network: 3 fully connected layers: (n_state_dims, 64), (64, 64), (64, n_actions). Learning rate=0.0005, optimizer=Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29770a-6b67-4b0e-af01-2c75eadf4a10",
   "metadata": {},
   "source": [
    "# Rewards plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f55b0-1f30-455c-a876-c5a113940781",
   "metadata": {},
   "source": [
    "Number of episodes required to solve the environment: 486.\n",
    "\n",
    "Weights can be found in ../weights/agents_weights.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6d5fd-3334-47d5-82e1-10e204d0873c",
   "metadata": {},
   "source": [
    "![title](./scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c12119-3242-439c-92de-ff52e531a92c",
   "metadata": {},
   "source": [
    "# Agent playing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1922b5a2-f2f4-4d33-8131-9204030299ba",
   "metadata": {},
   "source": [
    "![SegmentLocal](agent.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d04ea-92d9-494b-8970-bb835f7dbfee",
   "metadata": {},
   "source": [
    "# Ideas for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e939a801-ee3c-44d5-b9b4-4b2dd1358fb7",
   "metadata": {},
   "source": [
    "- Try out different neural network architectures: tune dimensiones, add/remove layers, adapt learning rate, adapt batch size.\n",
    "- Include DQN improvements: double DQN, prioritized experienced replay or Dueling DQN.\n",
    "- Try out Navigation_pixels with convolutional nets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents36",
   "language": "python",
   "name": "mlagents36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
